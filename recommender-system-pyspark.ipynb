{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pyspark\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import explode, struct, collect_list\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import json\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"spark_store_recs\"\n",
    "model_version = \"v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in table from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Cloud Dataprocs automatically propagated configurations to get the bucket and project for this cluster.\n",
    "bucket = sc._jsc.hadoopConfiguration().get(\"fs.gs.system.bucket\")\n",
    "project = sc._jsc.hadoopConfiguration().get(\"fs.gs.project.id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set an input directory for reading data from Bigquery.\n",
    "todays_date = datetime.strftime(datetime.today(), \"%Y-%m-%d-%H-%M-%S\")\n",
    "input_directory = \"gs://{}/store_recs/development/training/{}\".format(bucket, todays_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the configuration for importing data from BigQuery.\n",
    "# Specifically, make sure to set the project ID and bucket for Cloud Dataproc,\n",
    "# and the project ID, dataset, and table names for BigQuery.\n",
    "\n",
    "conf = {\n",
    "    # Input Parameters\n",
    "    \"mapred.bq.project.id\": project,\n",
    "    \"mapred.bq.gcs.bucket\": bucket,\n",
    "    \"mapred.bq.temp.gcs.path\": input_directory,\n",
    "    \"mapred.bq.input.project.id\": project,\n",
    "    \"mapred.bq.input.dataset.id\": \"data_science\",\n",
    "    \"mapred.bq.input.table.id\": \"store_recs_dev_train_90days\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data from BigQuery into Spark as an RDD.\n",
    "table_data = spark.sparkContext.newAPIHadoopRDD(\n",
    "    \"com.google.cloud.hadoop.io.bigquery.JsonTextBigQueryInputFormat\",\n",
    "    \"org.apache.hadoop.io.LongWritable\",\n",
    "    \"com.google.gson.JsonObject\",\n",
    "    conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the checkpoint directory\n",
    "sc.setCheckpointDir('checkpoint/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the JSON strings from the RDD.\n",
    "table_json = table_data.map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the JSON strings as a Spark Dataframe.\n",
    "transactions_data = spark.read.json(table_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users = transactions_data.select('user').distinct().count()\n",
    "n_stores = transactions_data.select('store_id').distinct().count()\n",
    "n_interactions = transactions_data.count()\n",
    "n_possible_interactions = n_users*n_stores\n",
    "sparsity = 100 - (n_interactions/n_possible_interactions)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check the data is dense enough to learn user preferences\n",
    "if sparsity > 99.50:\n",
    "    sys.exit(\"Matrix is too sparse.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transactions_data.persist().checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will index the ids and change the data type of the ratings because spark.ml only accepts Int types.\n",
    "# Make an indexer for users and items\n",
    "item_indexer, user_indexer = [StringIndexer(inputCol = x, outputCol = x+\"_index\").fit(transactions_data)\n",
    "            for x in list(set(transactions_data.columns) - set([\"rating\"]))]\n",
    "\n",
    "# Apply the indexer models to the DataFrame\n",
    "indexer_pipeline = Pipeline(stages = (item_indexer, user_indexer))\n",
    "transactions_index = indexer_pipeline.fit(transactions_data).transform(transactions_data)\n",
    "\n",
    "# Make rating an integer\n",
    "transactions_index = transactions_index.withColumn(\"rating\", transactions_index[\"rating\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transactions_index.persist().checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-03T21:51:34.730133Z",
     "start_time": "2018-08-03T14:51:34.726277-07:00"
    }
   },
   "source": [
    "## Split data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train, test, validate) = transactions_index.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Train: %d, Test: %d, Validate: %d\" % (train.count(), test.count(), validate.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "als = ALS(seed=0, implicitPrefs = True,\n",
    "           maxIter=20, regParam=0.1, alpha=1, rank=20, numUserBlocks = 50, numItemBlocks=50,\n",
    "          userCol = \"user_index\", itemCol=\"store_id_index\", ratingCol=\"rating\",\n",
    "          coldStartStrategy='nan', checkpointInterval=10, nonnegative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = als.fit(transactions_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.util import MLWriter\n",
    "outpath = \"gs://{}/store_recs/development/model/{}\".format(bucket, todays_date)\n",
    "model.write().save(outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName = \"mae\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print(\"MAE = \" + str(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(transactions_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.persist().checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend for all users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_200 = model.recommendForAllUsers(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_200.persist().checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Explode recommendations\n",
    "e_top_200 = top_200.withColumn('recommendations', explode('recommendations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expand store_index and rating from recommendations column\n",
    "e_top_200 = e_top_200.withColumn(\"store_index\", e_top_200.recommendations.store_id_index)\n",
    "e_top_200 = e_top_200.withColumn(\"rating\", e_top_200.recommendations.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e_top_200.persist().checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return original strings\n",
    "top_200_strings1 = IndexToString(inputCol = 'store_index', outputCol = 'store_id_original',\n",
    "                        labels = item_indexer.labels).transform(e_top_200)\n",
    "top_200_strings = IndexToString(inputCol = 'user_index', outputCol = 'user_original',\n",
    "                        labels = user_indexer.labels).transform(top_200_strings1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_200_strings.persist().checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collapse back to nested format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_200_recs = top_200_strings.select(\n",
    "    'user_original', struct(\n",
    "        'store_id_original', 'rating').alias('struct')).groupBy(\n",
    "    \"user_original\").agg(\n",
    "    collect_list(\"struct\").alias('recommendations'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write recs to GCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stage data formatted as newline-delimited JSON in Google Cloud Storage.\n",
    "output_directory = \"gs://{}/store_recs/development/predictions/{}\".format(bucket, todays_date)\n",
    "top_200_recs.write.format('json').save(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write evaluation metrics to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe to send with date\n",
    "from pyspark.sql import Row\n",
    "todays_date = datetime.strftime(datetime.today(), \"%Y-%m-%d-%H-%M-%S\")\n",
    "my_list = [(todays_date,mae,n_users,n_stores,sparsity,model_name,model_version)]\n",
    "my_rdd = sc.parallelize(my_list)\n",
    "date_metrics = my_rdd.map(lambda x: Row(date=x[0], mae=x[1], n_users=x[2],\n",
    "                                        n_stores=x[3], sparsity=x[4], model_name=x[5], model_version=x[6]))\n",
    "eval_metrics=sqlContext.createDataFrame(date_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_metrics = eval_metrics.coalesce(1) # Make one partition for easy storage navigation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new file in evaluation bucket \n",
    "output_directory = \"gs://{}/store_recs/development/evaluation/\".format(bucket)\n",
    "eval_metrics.write.format('json').mode('append').save(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shell out to bq CLI to perform BigQuery import - replace table with contents of bucket\n",
    "output_dataset='data_science'\n",
    "output_table='store_recs_eval'\n",
    "output_files = \"gs://{}/store_recs/development/evaluation/part-*\".format(bucket)\n",
    "\n",
    "subprocess.check_call(\n",
    "    'bq load --source_format NEWLINE_DELIMITED_JSON '\n",
    "    '--replace '\n",
    "    '--autodetect '\n",
    "    '{dataset}.{table} {files}'.format(\n",
    "        dataset=output_dataset, table=output_table, files=output_files\n",
    "    ).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the BigQuery table\n",
    "subprocess.check_call(\n",
    "\"bq rm -f data_science.store_recs_dev_train\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Check out predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_200_strings.createTempView('recs2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recs_view = spark.sql(\"SELECT store_id_original \\\n",
    "                      FROM recs2 \\\n",
    "                      WHERE user_original = 7761389455514436379\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recs_view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "248px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
